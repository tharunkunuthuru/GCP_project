import json
from datetime import datetime
import base64
from google.cloud import firestore, storage, bigquery
from google.cloud import kms

class JSONEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return json.JSONEncoder.default(self, obj)

def transfer_data(event, context):
    # Initialize Clients
    db = firestore.Client()
    storage_client = storage.Client()
    bigquery_client = bigquery.Client()
    kms_client = kms.KeyManagementServiceClient()

    # Configuration
    collection_name = 'machine_data'
    bucket_name = 'raw_data112'
    file_name = 'data.json'
    dataset_id = 'machine112'
    table_id = 'machine'
    crypto_key_id = 'projects/fleet-geode-425017-g6/locations/us-east1/keyRings/capstone/cryptoKeys/key1595'

    # Encryption Function
    def encrypt_value(value):
        plaintext = str(value).encode('utf-8')
        response = kms_client.encrypt(
            request={"name": crypto_key_id, "plaintext": plaintext}
        )
        return base64.b64encode(response.ciphertext).decode('utf-8')

    # Fetch Data from Firestore
    docs = db.collection(collection_name).stream()
    data = []
    for doc in docs:
        doc_dict = doc.to_dict()
        # Convert datetime objects to strings for JSON serialization
        for key, value in doc_dict.items():
            if isinstance(value, datetime):
                doc_dict[key] = value.isoformat()
        data.append(doc_dict)

    # Upload to Cloud Storage without encrypting
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(file_name)
    json_lines = "\n".join(json.dumps(record, cls=JSONEncoder) for record in data)
    blob.upload_from_string(data=json_lines, content_type='application/json')

    # Load into BigQuery with encryption during loading
    table_ref = bigquery_client.dataset(dataset_id).table(table_id)
    job_config = bigquery.LoadJobConfig(
        schema=[
            bigquery.SchemaField('humidity', 'STRING', description="Encrypted humidity value"),
            bigquery.SchemaField('machine', 'STRING'),
            bigquery.SchemaField('pressure', 'FLOAT'),
            bigquery.SchemaField('temperature', 'FLOAT'),
            bigquery.SchemaField('timestamp', 'TIMESTAMP')
        ],
        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,

        # Encryption configuration
        destination_encryption_configuration=bigquery.DestinationEncryptionConfiguration(
            kms_key_name=crypto_key_id
        ),
    )

    # Construct the query to encrypt the 'humidity' field
    encryption_query = f"""
    SELECT 
        AEAD.ENCRYPT_STRING(KEYS.NEW_WRAPPED_KEY('{crypto_key_id}'), CAST(humidity AS STRING)) AS humidity,
        machine, pressure, temperature, timestamp
    FROM `{dataset_id}.{table_id}_temp`
    """

    # Create a temporary table to store the unencrypted data
    temp_table_ref = bigquery_client.dataset(dataset_id).table(f"{table_id}_temp")

    # Load data into the temporary table
    uri = f'gs://{bucket_name}/{file_name}'
    load_job = bigquery_client.load_table_from_uri(uri, temp_table_ref, job_config=job_config)
    load_job.result()

    # Create a query job to encrypt and insert data into the main table
    query_job = bigquery_client.query(encryption_query, 
                                     destination=table_ref, 
                                     job_config=job_config)
    query_job.result()

    # Delete the temporary table
    bigquery_client.delete_table(temp_table_ref)
   
    print(f'Loaded {query_job.output_rows} rows into {dataset_id}.{table_id}')

