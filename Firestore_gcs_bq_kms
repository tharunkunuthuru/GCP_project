from google.cloud import firestore, storage, bigquery
import json
from datetime import datetime
import base64
from google.cloud import kms

# Custom JSON encoder for handling datetime objects
class JSONEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return json.JSONEncoder.default(self, obj)

def transfer_data(event, context):
    # Firestore, Storage, BigQuery, and KMS clients
    db = firestore.Client()
    storage_client = storage.Client()
    bigquery_client = bigquery.Client()
    kms_client = kms.KeyManagementServiceClient()

    # Configuration
    collection_name = 'machine_data'
    bucket_name = 'raw_data112'
    file_name = 'data.json'
    dataset_id = 'machine112'
    table_id = 'machine'
    crypto_key_id = 'projects/fleet-geode-425017-g6/locations/us-east1/keyRings/capstone/cryptoKeys/key1595' 
    ENCRYPTED_PREFIX = "ENCRYPTED_"

    # Encryption function
    def encrypt_value(value):
        plaintext = str(value).encode('utf-8')
        response = kms_client.encrypt(
            request={"name": crypto_key_id, "plaintext": plaintext}
        )
        return ENCRYPTED_PREFIX + base64.b64encode(response.ciphertext).decode('utf-8')

    # Get data from Firestore
    docs = db.collection(collection_name).stream()
    data = []
    for doc in docs:
        doc_dict = doc.to_dict()
        for key, value in doc_dict.items():
            if isinstance(value, datetime):
                doc_dict[key] = value.isoformat()
        doc_dict['timestamp'] = encrypt_value(doc_dict['timestamp'])
        data.append(doc_dict)

    # Upload data to Cloud Storage
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(file_name)
    json_lines = "\n".join(json.dumps(record, cls=JSONEncoder) for record in data)
    blob.upload_from_string(data=json_lines, content_type='application/json')

    # Load data into BigQuery
    table_ref = bigquery_client.dataset(dataset_id).table(table_id)
    job_config = bigquery.LoadJobConfig(
        schema=[
            bigquery.SchemaField('humidity', 'FLOAT'),
            bigquery.SchemaField('machine', 'STRING'),
            bigquery.SchemaField('pressure', 'FLOAT'),
            bigquery.SchemaField('temperature', 'STRING'),  
            bigquery.SchemaField('timestamp', 'TIMESTAMP')
        ],
        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
    )

    uri = f'gs://{bucket_name}/{file_name}'
    load_job = bigquery_client.load_table_from_uri(uri, table_ref, job_config=job_config)
    load_job.result()

    print(f'Loaded {load_job.output_rows} rows into {dataset_id}.{table_id}')
